{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PtEeL-5IF77",
        "outputId": "b194280c-d273-4a4b-d7e7-6f31366cccb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting Twisted>=18.9.0 (from scrapy)\n",
            "  Downloading twisted-24.7.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (43.0.0)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (71.0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.4)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.17.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.0)\n",
            "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.7)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.15.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=24.7.0->Twisted>=18.9.0->scrapy) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n",
            "Downloading Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.1-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
            "Downloading twisted-24.7.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.7.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.1 jmespath-1.0.1 parsel-1.9.1 protego-0.3.1 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.11.2 service-identity-24.1.0 tldextract-5.1.2 w3lib-2.2.1 zope.interface-7.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**crawler.py**"
      ],
      "metadata": {
        "id": "PtilTXoqFrcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YYXhASNfFqQx"
      },
      "outputs": [],
      "source": [
        "\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "\n",
        "class CrawlingSpider(CrawlSpider):\n",
        "  name=\"mycrawl\"\n",
        "  allowed_domains=[\"toscrape.com\"]\n",
        "  start_urls=[\"https://books.toscrape.com/\"]\n",
        "  PROXY_SERVER=\"127.0.0.1\"\n",
        "  rules= (\n",
        "      Rule(LinkExtractor(allow=r\"catalogue/category\")),\n",
        "      Rule(LinkExtractor(allow=r\"catalogue\", deny=r\"category\"), callback=\"parse_item\")\n",
        "\n",
        "  )\n",
        "  def parse_item(self, response):\n",
        "      yield {\n",
        "      \"title\": response.css(\".product_main h1::text\").get(),\n",
        "      \"price\": response.css(\".price_color::text\").get(),\n",
        "      \"availability\": response.css(\".availability::text\").get().replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
        "      }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**settings.py**"
      ],
      "metadata": {
        "id": "mfoK44_3Fv0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrapy settings for crawling project\n",
        "#\n",
        "# For simplicity, this file contains only settings considered important or\n",
        "# commonly used. You can find more settings consulting the documentation:\n",
        "#\n",
        "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "import scrapy.downloadermiddlewares.httpproxy\n",
        "\n",
        "BOT_NAME = \"crawling\"\n",
        "\n",
        "SPIDER_MODULES = [\"crawling.spiders\"]\n",
        "NEWSPIDER_MODULE = \"crawling.spiders\"\n",
        "\n",
        "\n",
        "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
        "#USER_AGENT = \"crawling (+http://www.yourdomain.com)\"\n",
        "\n",
        "# Obey robots.txt rules\n",
        "ROBOTSTXT_OBEY = True\n",
        "\n",
        "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
        "#CONCURRENT_REQUESTS = 32\n",
        "\n",
        "# Configure a delay for requests for the same website (default: 0)\n",
        "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
        "# See also autothrottle settings and docs\n",
        "#DOWNLOAD_DELAY = 3\n",
        "# The download delay setting will honor only one of:\n",
        "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
        "#CONCURRENT_REQUESTS_PER_IP = 16\n",
        "\n",
        "# Disable cookies (enabled by default)\n",
        "#COOKIES_ENABLED = False\n",
        "\n",
        "# Disable Telnet Console (enabled by default)\n",
        "#TELNETCONSOLE_ENABLED = False\n",
        "\n",
        "# Override the default request headers:\n",
        "#DEFAULT_REQUEST_HEADERS = {\n",
        "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "#    \"Accept-Language\": \"en\",\n",
        "#}\n",
        "\n",
        "# Enable or disable spider middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "#SPIDER_MIDDLEWARES = {\n",
        "#    \"crawling.middlewares.CrawlingSpiderMiddleware\": 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable downloader middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "DOWNLOADER_MIDDLEWARES = {\n",
        "  \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\":1,\n",
        "   \"crawling.middlewares.CrawlingDownloaderMiddleware\": 543,\n",
        "}\n",
        "\n",
        "# Enable or disable extensions\n",
        "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
        "#EXTENSIONS = {\n",
        "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
        "#}\n",
        "\n",
        "# Configure item pipelines\n",
        "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "#ITEM_PIPELINES = {\n",
        "#    \"crawling.pipelines.CrawlingPipeline\": 300,\n",
        "#}\n",
        "\n",
        "# Enable and configure the AutoThrottle extension (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
        "#AUTOTHROTTLE_ENABLED = True\n",
        "# The initial download delay\n",
        "#AUTOTHROTTLE_START_DELAY = 5\n",
        "# The maximum download delay to be set in case of high latencies\n",
        "#AUTOTHROTTLE_MAX_DELAY = 60\n",
        "# The average number of requests Scrapy should be sending in parallel to\n",
        "# each remote server\n",
        "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
        "# Enable showing throttling stats for every response received:\n",
        "#AUTOTHROTTLE_DEBUG = False\n",
        "\n",
        "# Enable and configure HTTP caching (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
        "#HTTPCACHE_ENABLED = True\n",
        "#HTTPCACHE_EXPIRATION_SECS = 0\n",
        "#HTTPCACHE_DIR = \"httpcache\"\n",
        "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
        "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
        "\n",
        "# Set settings whose default value is deprecated to a future-proof value\n",
        "REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
        "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
        "FEED_EXPORT_ENCODING = \"utf-8\"\n"
      ],
      "metadata": {
        "id": "C0HWa6TdF3l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**middleware.py**"
      ],
      "metadata": {
        "id": "hEsH-cO-F8s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define here the models for your spider middleware\n",
        "#\n",
        "# See documentation in:\n",
        "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "\n",
        "from scrapy import signals\n",
        "\n",
        "# useful for handling different item types with a single interface\n",
        "from itemadapter import is_item, ItemAdapter\n",
        "\n",
        "\n",
        "class CrawlingSpiderMiddleware:\n",
        "    # Not all methods need to be defined. If a method is not defined,\n",
        "    # scrapy acts as if the spider middleware does not modify the\n",
        "    # passed objects.\n",
        "\n",
        "    @classmethod\n",
        "    def from_crawler(cls, crawler):\n",
        "        # This method is used by Scrapy to create your spiders.\n",
        "        s = cls()\n",
        "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
        "        return s\n",
        "\n",
        "    def process_spider_input(self, response, spider):\n",
        "        # Called for each response that goes through the spider\n",
        "        # middleware and into the spider.\n",
        "\n",
        "        # Should return None or raise an exception.\n",
        "        return None\n",
        "\n",
        "    def process_spider_output(self, response, result, spider):\n",
        "        # Called with the results returned from the Spider, after\n",
        "        # it has processed the response.\n",
        "\n",
        "        # Must return an iterable of Request, or item objects.\n",
        "        for i in result:\n",
        "            yield i\n",
        "\n",
        "    def process_spider_exception(self, response, exception, spider):\n",
        "        # Called when a spider or process_spider_input() method\n",
        "        # (from other spider middleware) raises an exception.\n",
        "\n",
        "        # Should return either None or an iterable of Request or item objects.\n",
        "        pass\n",
        "\n",
        "    def process_start_requests(self, start_requests, spider):\n",
        "        # Called with the start requests of the spider, and works\n",
        "        # similarly to the process_spider_output() method, except\n",
        "        # that it doesn’t have a response associated.\n",
        "\n",
        "        # Must return only requests (not items).\n",
        "        for r in start_requests:\n",
        "            yield r\n",
        "\n",
        "    def spider_opened(self, spider):\n",
        "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n",
        "\n",
        "\n",
        "class CrawlingDownloaderMiddleware:\n",
        "    # Not all methods need to be defined. If a method is not defined,\n",
        "    # scrapy acts as if the downloader middleware does not modify the\n",
        "    # passed objects.\n",
        "\n",
        "    @classmethod\n",
        "    def from_crawler(cls, crawler):\n",
        "        # This method is used by Scrapy to create your spiders.\n",
        "        s = cls()\n",
        "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
        "        return s\n",
        "\n",
        "    def process_request(self, request, spider):\n",
        "        # Called for each request that goes through the downloader\n",
        "        # middleware.\n",
        "\n",
        "        # Must either:\n",
        "        # - return None: continue processing this request\n",
        "        # - or return a Response object\n",
        "        # - or return a Request object\n",
        "        # - or raise IgnoreRequest: process_exception() methods of\n",
        "        #   installed downloader middleware will be called\n",
        "        request.meta['Proxy']=\"127.0.0.1\"\n",
        "        return None\n",
        "\n",
        "    def process_response(self, request, response, spider):\n",
        "        # Called with the response returned from the downloader.\n",
        "\n",
        "        # Must either;\n",
        "        # - return a Response object\n",
        "        # - return a Request object\n",
        "        # - or raise IgnoreRequest\n",
        "        return response\n",
        "\n",
        "    def process_exception(self, request, exception, spider):\n",
        "        # Called when a download handler or a process_request()\n",
        "        # (from other downloader middleware) raises an exception.\n",
        "\n",
        "        # Must either:\n",
        "        # - return None: continue processing this exception\n",
        "        # - return a Response object: stops process_exception() chain\n",
        "        # - return a Request object: stops process_exception() chain\n",
        "        pass\n",
        "\n",
        "    def spider_opened(self, spider):\n",
        "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n"
      ],
      "metadata": {
        "id": "8_3hD89qF_xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pipelines.py**"
      ],
      "metadata": {
        "id": "XjkllWtBGEna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your item pipelines here\n",
        "#\n",
        "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
        "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "\n",
        "\n",
        "# useful for handling different item types with a single interface\n",
        "from itemadapter import ItemAdapter\n",
        "\n",
        "\n",
        "class CrawlyPipeline:\n",
        "    def process_item(self, item, spider):\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "w4c4RX6gGKLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**items.py**"
      ],
      "metadata": {
        "id": "v533i117GOQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define here the models for your scraped items\n",
        "#\n",
        "# See documentation in:\n",
        "# https://docs.scrapy.org/en/latest/topics/items.html\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class CrawlyItem(scrapy.Item):\n",
        "    # define the fields for your item here like:\n",
        "    # name = scrapy.Field()\n",
        "    pass"
      ],
      "metadata": {
        "id": "D7BL8gTcGSg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "basically change the ip adress and proxy coz once u start crawling u might be blocked by the site. here i have used the local host.\n",
        "\n",
        "Just paste the link to which website u want to crawl, here i have used **\"toscrape.com\"**\n",
        "\n",
        "\n",
        "**To run the crawling use command in terminal: scrapy crawl (Filename)**\n",
        "\n",
        "\n",
        "**to get the json file as scrap use command in terminal:- scrapy crawl (Filename) - o- json**"
      ],
      "metadata": {
        "id": "PcauyLwKGZK9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jlxAV11GUbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}